{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Forecast Reconciliation with Uncertainty Quantification\n",
    "\n",
    "This notebook explores the M5 dataset and demonstrates the hierarchical forecasting approach with uncertainty quantification.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project implements a novel ensemble approach that combines:\n",
    "- **Statistical models**: ETS and ARIMA for baseline forecasting\n",
    "- **Deep learning models**: Temporal Fusion Transformer and N-BEATS\n",
    "- **Hierarchical reconciliation**: Probabilistic reconciliation that preserves prediction intervals\n",
    "- **Uncertainty quantification**: Coherent prediction intervals across all hierarchy levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Project imports\n",
    "sys.path.append('../src')\n",
    "from hierarchical_forecast_reconciliation_with_uncertainty_quantification.utils.config import (\n",
    "    load_config, set_random_seeds, setup_logging\n",
    ")\n",
    "from hierarchical_forecast_reconciliation_with_uncertainty_quantification.data.loader import (\n",
    "    M5DataLoader, HierarchicalDataBuilder\n",
    ")\n",
    "from hierarchical_forecast_reconciliation_with_uncertainty_quantification.data.preprocessing import (\n",
    "    M5Preprocessor, HierarchyBuilder\n",
    ")\n",
    "from hierarchical_forecast_reconciliation_with_uncertainty_quantification.models.model import (\n",
    "    StatisticalForecaster, HierarchicalEnsembleForecaster\n",
    ")\n",
    "from hierarchical_forecast_reconciliation_with_uncertainty_quantification.evaluation.metrics import (\n",
    "    HierarchicalMetrics\n",
    ")\n",
    "\n",
    "# Configure environment\n",
    "warnings.filterwarnings('ignore')\n",
    "set_random_seeds(42)\n",
    "setup_logging(level='INFO', console=True)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('../configs/default.yaml')\n",
    "\n",
    "# Update data path for demo (you would set this to your actual M5 data path)\n",
    "# config['data']['path'] = '/path/to/m5/data'\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"- Aggregation levels: {len(config['data']['aggregation_levels'])}\")\n",
    "print(f\"- Statistical models: {list(config['models']['statistical'].keys())}\")\n",
    "print(f\"- Deep learning models: {list(config['models']['deep_learning'].keys())}\")\n",
    "print(f\"- Ensemble weights: {config['ensemble']['weights']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic M5-like Data for Demonstration\n",
    "\n",
    "Since the actual M5 dataset is large, we'll generate synthetic data that follows similar patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_m5_data(n_days=1969, n_items=50, seasonal_strength=0.3, trend_strength=0.1):\n",
    "    \"\"\"\n",
    "    Generate synthetic M5-like hierarchical time series data.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Date range\n",
    "    dates = pd.date_range('2016-01-01', periods=n_days, freq='D')\n",
    "    \n",
    "    # Hierarchy structure\n",
    "    states = ['CA', 'TX', 'WI']\n",
    "    categories = ['FOODS', 'HOBBIES', 'HOUSEHOLD']\n",
    "    stores_per_state = {'CA': 4, 'TX': 3, 'WI': 3}\n",
    "    items_per_category = n_items // len(categories)\n",
    "    \n",
    "    data_rows = []\n",
    "    \n",
    "    for state in states:\n",
    "        for store_idx in range(stores_per_state[state]):\n",
    "            store_id = f\"{state}_{store_idx + 1}\"\n",
    "            \n",
    "            for cat in categories:\n",
    "                for item_idx in range(items_per_category):\n",
    "                    item_id = f\"{cat}_{item_idx + 1:03d}\"\n",
    "                    series_id = f\"{item_id}_{store_id}_validation\"\n",
    "                    \n",
    "                    # Generate base pattern\n",
    "                    t = np.arange(n_days)\n",
    "                    \n",
    "                    # Trend component\n",
    "                    trend = trend_strength * t / 365  # Yearly trend\n",
    "                    \n",
    "                    # Seasonal components\n",
    "                    weekly_seasonal = seasonal_strength * np.sin(2 * np.pi * t / 7)\n",
    "                    yearly_seasonal = seasonal_strength * 0.5 * np.sin(2 * np.pi * t / 365.25)\n",
    "                    \n",
    "                    # Base level (varies by category)\n",
    "                    base_levels = {'FOODS': 5, 'HOBBIES': 2, 'HOUSEHOLD': 3}\n",
    "                    base_level = base_levels[cat]\n",
    "                    \n",
    "                    # Store effect\n",
    "                    store_effect = np.random.normal(1, 0.2)\n",
    "                    \n",
    "                    # Noise\n",
    "                    noise = np.random.normal(0, 0.5, n_days)\n",
    "                    \n",
    "                    # Combine components\n",
    "                    sales = np.maximum(0, \n",
    "                        base_level * store_effect + trend + weekly_seasonal + yearly_seasonal + noise\n",
    "                    )\n",
    "                    \n",
    "                    # Add some intermittency\n",
    "                    intermittency = np.random.binomial(1, 0.85, n_days)  # 15% zero sales\n",
    "                    sales = sales * intermittency\n",
    "                    \n",
    "                    for day, (date, sale) in enumerate(zip(dates, sales)):\n",
    "                        data_rows.append({\n",
    "                            'id': series_id,\n",
    "                            'item_id': item_id,\n",
    "                            'dept_id': cat,\n",
    "                            'cat_id': cat,\n",
    "                            'store_id': store_id,\n",
    "                            'state_id': state,\n",
    "                            'date': date,\n",
    "                            'sales': sale,\n",
    "                            'd': f'd_{day + 1}'\n",
    "                        })\n",
    "    \n",
    "    return pd.DataFrame(data_rows)\n",
    "\n",
    "# Generate synthetic data\n",
    "print(\"Generating synthetic M5-like data...\")\n",
    "synthetic_data = generate_synthetic_m5_data()\n",
    "\n",
    "print(f\"Generated data shape: {synthetic_data.shape}\")\n",
    "print(f\"Date range: {synthetic_data['date'].min()} to {synthetic_data['date'].max()}\")\n",
    "print(f\"Number of series: {synthetic_data['id'].nunique()}\")\n",
    "print(f\"Hierarchy levels:\")\n",
    "print(f\"  - States: {synthetic_data['state_id'].nunique()}\")\n",
    "print(f\"  - Stores: {synthetic_data['store_id'].nunique()}\")\n",
    "print(f\"  - Categories: {synthetic_data['cat_id'].nunique()}\")\n",
    "print(f\"  - Items: {synthetic_data['item_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Sales Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate sales by date\n",
    "daily_total = synthetic_data.groupby('date')['sales'].sum().reset_index()\n",
    "\n",
    "# Create interactive plot\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=daily_total['date'],\n",
    "    y=daily_total['sales'],\n",
    "    mode='lines',\n",
    "    name='Total Sales',\n",
    "    line=dict(color='blue', width=1)\n",
    "))\n",
    "\n",
    "# Add 30-day moving average\n",
    "daily_total['ma_30'] = daily_total['sales'].rolling(window=30).mean()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=daily_total['date'],\n",
    "    y=daily_total['ma_30'],\n",
    "    mode='lines',\n",
    "    name='30-day MA',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Total Daily Sales Over Time',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Sales Units',\n",
    "    height=500,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nDaily Sales Statistics:\")\n",
    "print(daily_total['sales'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales by Hierarchy Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by state\n",
    "state_sales = synthetic_data.groupby(['date', 'state_id'])['sales'].sum().reset_index()\n",
    "\n",
    "fig = px.line(state_sales, x='date', y='sales', color='state_id',\n",
    "              title='Sales by State Over Time',\n",
    "              labels={'sales': 'Sales Units', 'date': 'Date'})\n",
    "fig.update_layout(height=400)\n",
    "fig.show()\n",
    "\n",
    "# Sales by category\n",
    "category_sales = synthetic_data.groupby(['date', 'cat_id'])['sales'].sum().reset_index()\n",
    "\n",
    "fig = px.line(category_sales, x='date', y='sales', color='cat_id',\n",
    "              title='Sales by Category Over Time',\n",
    "              labels={'sales': 'Sales Units', 'date': 'Date'})\n",
    "fig.update_layout(height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time features\n",
    "synthetic_data['weekday'] = synthetic_data['date'].dt.day_name()\n",
    "synthetic_data['month'] = synthetic_data['date'].dt.month\n",
    "synthetic_data['week_of_year'] = synthetic_data['date'].dt.isocalendar().week\n",
    "\n",
    "# Weekly patterns\n",
    "weekly_pattern = synthetic_data.groupby('weekday')['sales'].mean().reset_index()\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekly_pattern['weekday'] = pd.Categorical(weekly_pattern['weekday'], categories=weekday_order, ordered=True)\n",
    "weekly_pattern = weekly_pattern.sort_values('weekday')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Weekly pattern\n",
    "axes[0, 0].bar(weekly_pattern['weekday'], weekly_pattern['sales'], color='skyblue')\n",
    "axes[0, 0].set_title('Average Sales by Day of Week')\n",
    "axes[0, 0].set_ylabel('Sales Units')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Monthly pattern\n",
    "monthly_pattern = synthetic_data.groupby('month')['sales'].mean().reset_index()\n",
    "axes[0, 1].plot(monthly_pattern['month'], monthly_pattern['sales'], marker='o', color='orange')\n",
    "axes[0, 1].set_title('Average Sales by Month')\n",
    "axes[0, 1].set_xlabel('Month')\n",
    "axes[0, 1].set_ylabel('Sales Units')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Sales distribution\n",
    "axes[1, 0].hist(synthetic_data[synthetic_data['sales'] > 0]['sales'], bins=50, alpha=0.7, color='green')\n",
    "axes[1, 0].set_title('Sales Distribution (Non-zero)')\n",
    "axes[1, 0].set_xlabel('Sales Units')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Zero sales percentage\n",
    "zero_sales_pct = synthetic_data.groupby('id').apply(\n",
    "    lambda x: (x['sales'] == 0).mean() * 100\n",
    ").reset_index(name='zero_pct')\n",
    "\n",
    "axes[1, 1].hist(zero_sales_pct['zero_pct'], bins=20, alpha=0.7, color='coral')\n",
    "axes[1, 1].set_title('Distribution of Zero Sales Percentage by Series')\n",
    "axes[1, 1].set_xlabel('Zero Sales Percentage')\n",
    "axes[1, 1].set_ylabel('Number of Series')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOverall zero sales rate: {(synthetic_data['sales'] == 0).mean():.1%}\")\n",
    "print(f\"Average zero sales rate per series: {zero_sales_pct['zero_pct'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchy Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hierarchical aggregations\n",
    "hierarchy_builder = HierarchicalDataBuilder(config['data']['aggregation_levels'])\n",
    "hierarchy_data = hierarchy_builder.build_hierarchy(synthetic_data)\n",
    "\n",
    "# Analyze hierarchy structure\n",
    "matrix_builder = HierarchyBuilder(config['data']['aggregation_levels'])\n",
    "structure_info = matrix_builder.get_hierarchy_structure(hierarchy_data)\n",
    "\n",
    "print(\"Hierarchy Structure:\")\n",
    "print(\"-\" * 50)\n",
    "total_series = 0\n",
    "for level, info in structure_info.items():\n",
    "    print(f\"{level:15s}: {info['n_series']:4d} series, {info['total_observations']:7d} observations\")\n",
    "    total_series += info['n_series']\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Total':15s}: {total_series:4d} series across all levels\")\n",
    "\n",
    "# Visualize hierarchy structure\n",
    "levels = list(structure_info.keys())\n",
    "n_series = [structure_info[level]['n_series'] for level in levels]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "bars = ax.bar(levels, n_series, color='lightblue', edgecolor='navy', alpha=0.7)\n",
    "ax.set_title('Number of Series by Hierarchy Level', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Number of Series')\n",
    "ax.set_xlabel('Hierarchy Level')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, n_series):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(n_series) * 0.01,\n",
    "            str(value), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = M5Preprocessor(\n",
    "    scaling_method=\"standard\",\n",
    "    handle_zeros=\"log1p\",\n",
    "    outlier_threshold=3.0\n",
    ")\n",
    "\n",
    "# Preprocess data\n",
    "print(\"Preprocessing data...\")\n",
    "processed_data = preprocessor.fit_transform(synthetic_data)\n",
    "\n",
    "print(f\"Original data shape: {synthetic_data.shape}\")\n",
    "print(f\"Processed data shape: {processed_data.shape}\")\n",
    "print(f\"Added features: {processed_data.shape[1] - synthetic_data.shape[1]}\")\n",
    "\n",
    "# Show new features\n",
    "new_features = [col for col in processed_data.columns if col not in synthetic_data.columns]\n",
    "print(f\"\\nNew features added: {new_features[:10]}...\")  # Show first 10\n",
    "\n",
    "# Create train/validation/test splits\n",
    "train_data, val_data, test_data = preprocessor.create_train_test_split(\n",
    "    processed_data,\n",
    "    train_days=config['data']['train_days'],\n",
    "    validation_days=config['data']['validation_days'],\n",
    "    test_days=config['data']['test_days']\n",
    ")\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"  Train: {train_data.shape[0]:,} observations\")\n",
    "print(f\"  Validation: {val_data.shape[0]:,} observations\")\n",
    "print(f\"  Test: {test_data.shape[0]:,} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate statistical forecaster with a subset of data\n",
    "# Select a few representative series for demonstration\n",
    "demo_series = train_data['id'].unique()[:5]\n",
    "demo_train = train_data[train_data['id'].isin(demo_series)]\n",
    "\n",
    "print(f\"Training statistical model on {len(demo_series)} series...\")\n",
    "\n",
    "# Initialize and fit statistical forecaster\n",
    "stat_forecaster = StatisticalForecaster(\n",
    "    method=\"ets\",\n",
    "    ets_config=config['models']['statistical']['ets']\n",
    ")\n",
    "\n",
    "try:\n",
    "    stat_forecaster.fit(demo_train)\n",
    "    \n",
    "    # Generate predictions\n",
    "    horizon = 28\n",
    "    stat_predictions = stat_forecaster.predict(\n",
    "        horizon=horizon,\n",
    "        return_intervals=True,\n",
    "        confidence_levels=[0.1, 0.05]\n",
    "    )\n",
    "    \n",
    "    print(\"Statistical model trained successfully!\")\n",
    "    print(f\"Generated forecasts for {len(stat_predictions['forecasts'])} series\")\n",
    "    \n",
    "    # Visualize predictions for first series\n",
    "    series_id = list(stat_predictions['forecasts'].keys())[0]\n",
    "    \n",
    "    # Get historical data for this series\n",
    "    hist_data = demo_train[demo_train['id'] == series_id].sort_values('date')\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    # Plot historical data (last 100 points)\n",
    "    hist_plot = hist_data.tail(100)\n",
    "    ax.plot(range(len(hist_plot)), hist_plot['sales'], 'b-', label='Historical', alpha=0.8)\n",
    "    \n",
    "    # Plot forecast\n",
    "    forecast_start = len(hist_plot)\n",
    "    forecast_range = range(forecast_start, forecast_start + horizon)\n",
    "    \n",
    "    forecast = stat_predictions['forecasts'][series_id]\n",
    "    ax.plot(forecast_range, forecast, 'r-', label='Forecast', linewidth=2)\n",
    "    \n",
    "    # Plot confidence intervals\n",
    "    if 'lower_90' in stat_predictions:\n",
    "        lower_90 = stat_predictions['lower_90'][series_id]\n",
    "        upper_90 = stat_predictions['upper_90'][series_id]\n",
    "        ax.fill_between(forecast_range, lower_90, upper_90, alpha=0.3, color='red', label='90% CI')\n",
    "    \n",
    "    ax.axvline(x=forecast_start, color='black', linestyle='--', alpha=0.7, label='Forecast Start')\n",
    "    ax.set_title(f'Statistical Model Forecast: {series_id[:30]}...')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Sales')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in statistical modeling: {e}\")\n",
    "    print(\"This is expected in a demo environment without full dependencies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate evaluation metrics with synthetic predictions\n",
    "evaluator = HierarchicalMetrics()\n",
    "\n",
    "# Generate synthetic predictions and actuals for demonstration\n",
    "np.random.seed(42)\n",
    "demo_horizon = 28\n",
    "\n",
    "demo_predictions = {}\n",
    "demo_actuals = {}\n",
    "demo_intervals = {'lower_90': {}, 'upper_90': {}, 'lower_95': {}, 'upper_95': {}}\n",
    "\n",
    "for series_id in demo_series:\n",
    "    # Generate realistic predictions (with some error)\n",
    "    true_values = np.random.uniform(2, 8, demo_horizon)\n",
    "    predictions = true_values + np.random.normal(0, 0.5, demo_horizon)\n",
    "    \n",
    "    demo_predictions[series_id] = predictions\n",
    "    demo_actuals[series_id] = true_values\n",
    "    \n",
    "    # Generate prediction intervals\n",
    "    demo_intervals['lower_90'][series_id] = predictions - 1.645 * 0.5\n",
    "    demo_intervals['upper_90'][series_id] = predictions + 1.645 * 0.5\n",
    "    demo_intervals['lower_95'][series_id] = predictions - 1.96 * 0.5\n",
    "    demo_intervals['upper_95'][series_id] = predictions + 1.96 * 0.5\n",
    "\n",
    "# Compute metrics\n",
    "metrics = evaluator.compute_all_metrics(\n",
    "    predictions=demo_predictions,\n",
    "    actuals=demo_actuals,\n",
    "    intervals=demo_intervals,\n",
    "    confidence_levels=[0.1, 0.05]\n",
    ")\n",
    "\n",
    "print(\"Evaluation Metrics (Synthetic Example):\")\n",
    "print(\"=\" * 50)\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"{metric_name:25s}: {value:.4f}\")\n",
    "\n",
    "# Generate performance report\n",
    "target_metrics = {\n",
    "    \"WRMSSE\": 0.52,\n",
    "    \"coverage_90\": 0.88,\n",
    "    \"coverage_95\": 0.94\n",
    "}\n",
    "\n",
    "report = evaluator.create_performance_report(metrics, target_metrics)\n",
    "print(\"\\n\" + report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Reconciliation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the concept of hierarchical reconciliation\n",
    "from hierarchical_forecast_reconciliation_with_uncertainty_quantification.models.model import ProbabilisticReconciler\n",
    "\n",
    "# Build aggregation matrix\n",
    "aggregation_matrix = matrix_builder.build_aggregation_matrix(hierarchy_data)\n",
    "\n",
    "print(f\"Aggregation matrix shape: {aggregation_matrix.shape}\")\n",
    "print(f\"Matrix density: {aggregation_matrix.nnz / (aggregation_matrix.shape[0] * aggregation_matrix.shape[1]):.3f}\")\n",
    "\n",
    "# Visualize a small portion of the aggregation matrix\n",
    "# Show first 20 rows and columns as dense matrix\n",
    "matrix_sample = aggregation_matrix[:20, :20].toarray()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "im = ax.imshow(matrix_sample, cmap='Blues', aspect='auto')\n",
    "ax.set_title('Aggregation Matrix Sample (First 20x20)', fontsize=14)\n",
    "ax.set_xlabel('Bottom-level Series')\n",
    "ax.set_ylabel('Aggregated Series')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Aggregation Weight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAggregation Matrix Properties:\")\n",
    "print(f\"- Each row represents an aggregated series\")\n",
    "print(f\"- Each column represents a bottom-level series\")\n",
    "print(f\"- Non-zero entries indicate which bottom series contribute to each aggregate\")\n",
    "print(f\"- The matrix enforces hierarchical coherence: aggregates = sum of components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconciliation Impact Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the impact of reconciliation on forecast coherence\n",
    "\n",
    "# Create synthetic incoherent forecasts\n",
    "n_series = aggregation_matrix.shape[0]\n",
    "n_bottom = aggregation_matrix.shape[1]\n",
    "\n",
    "# Generate incoherent base forecasts\n",
    "np.random.seed(123)\n",
    "incoherent_forecasts = {}\n",
    "for i in range(n_series):\n",
    "    series_id = f\"series_{i}\"\n",
    "    incoherent_forecasts[series_id] = np.random.uniform(50, 150, 28)\n",
    "\n",
    "# Initialize reconciler and fit\n",
    "reconciler = ProbabilisticReconciler(\n",
    "    method=\"probabilistic_mint\",\n",
    "    weights=\"ols\"\n",
    ")\n",
    "\n",
    "reconciler.fit(aggregation_matrix)\n",
    "\n",
    "# Apply reconciliation\n",
    "reconciled_forecasts, _ = reconciler.reconcile(incoherent_forecasts)\n",
    "\n",
    "# Compute coherence scores\n",
    "coherence_before = reconciler.compute_coherence_score(incoherent_forecasts)\n",
    "coherence_after = reconciler.compute_coherence_score(reconciled_forecasts)\n",
    "\n",
    "print(\"Reconciliation Impact:\")\n",
    "print(f\"Coherence before reconciliation: {coherence_before:.4f}\")\n",
    "print(f\"Coherence after reconciliation:  {coherence_after:.4f}\")\n",
    "print(f\"Improvement: {coherence_after - coherence_before:.4f}\")\n",
    "\n",
    "# Visualize the improvement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Coherence comparison\n",
    "coherence_data = ['Before Reconciliation', 'After Reconciliation']\n",
    "coherence_values = [coherence_before, coherence_after]\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "\n",
    "bars = axes[0].bar(coherence_data, coherence_values, color=colors, alpha=0.8)\n",
    "axes[0].set_title('Hierarchical Coherence Improvement')\n",
    "axes[0].set_ylabel('Coherence Score (0-1)')\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, coherence_values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Forecast adjustment distribution\n",
    "adjustments = []\n",
    "for series_id in incoherent_forecasts.keys():\n",
    "    if series_id in reconciled_forecasts:\n",
    "        original = incoherent_forecasts[series_id]\n",
    "        reconciled = reconciled_forecasts[series_id]\n",
    "        relative_change = np.mean(np.abs(reconciled - original) / (np.abs(original) + 1e-8))\n",
    "        adjustments.append(relative_change)\n",
    "\n",
    "axes[1].hist(adjustments, bins=20, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "axes[1].set_title('Distribution of Forecast Adjustments')\n",
    "axes[1].set_xlabel('Relative Adjustment Size')\n",
    "axes[1].set_ylabel('Number of Series')\n",
    "axes[1].axvline(np.mean(adjustments), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(adjustments):.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMean relative adjustment: {np.mean(adjustments):.3f}\")\n",
    "print(f\"This shows how much forecasts are typically adjusted to maintain coherence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual representation of the model architecture\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch, ConnectionPatch\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "\n",
    "# Define colors\n",
    "colors = {\n",
    "    'data': 'lightblue',\n",
    "    'statistical': 'lightgreen', \n",
    "    'deep_learning': 'lightyellow',\n",
    "    'ensemble': 'lightcoral',\n",
    "    'reconciliation': 'lightpink',\n",
    "    'output': 'lightgray'\n",
    "}\n",
    "\n",
    "# Helper function to create boxes\n",
    "def create_box(ax, x, y, width, height, text, color, fontsize=10):\n",
    "    box = FancyBboxPatch(\n",
    "        (x, y), width, height,\n",
    "        boxstyle=\"round,pad=0.1\",\n",
    "        facecolor=color,\n",
    "        edgecolor='black',\n",
    "        linewidth=1\n",
    "    )\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + width/2, y + height/2, text, ha='center', va='center', \n",
    "            fontsize=fontsize, fontweight='bold', wrap=True)\n",
    "    return box\n",
    "\n",
    "# Data layer\n",
    "create_box(ax, 0.5, 10, 9, 1, 'M5 Hierarchical Time Series Data', colors['data'], 12)\n",
    "\n",
    "# Statistical models\n",
    "create_box(ax, 0.5, 8, 2, 1, 'ETS\\nModel', colors['statistical'])\n",
    "create_box(ax, 2.8, 8, 2, 1, 'ARIMA\\nModel', colors['statistical'])\n",
    "\n",
    "# Deep learning models\n",
    "create_box(ax, 5.6, 8, 2, 1, 'Temporal Fusion\\nTransformer', colors['deep_learning'])\n",
    "create_box(ax, 7.9, 8, 2, 1, 'N-BEATS\\nModel', colors['deep_learning'])\n",
    "\n",
    "# Ensemble layer\n",
    "create_box(ax, 2, 6, 6, 1, 'Weighted Ensemble Combination', colors['ensemble'], 12)\n",
    "\n",
    "# Reconciliation layer\n",
    "create_box(ax, 1, 4, 8, 1, 'Probabilistic Hierarchical Reconciliation\\n(MinT with Uncertainty Preservation)', \n",
    "           colors['reconciliation'], 11)\n",
    "\n",
    "# Output layer\n",
    "create_box(ax, 0.5, 2, 4.3, 1, 'Coherent Point\\nForecasts', colors['output'])\n",
    "create_box(ax, 5.2, 2, 4.3, 1, 'Coherent Prediction\\nIntervals', colors['output'])\n",
    "\n",
    "# Add arrows\n",
    "arrow_props = dict(arrowstyle='->', lw=2, color='black')\n",
    "\n",
    "# Data to models\n",
    "ax.annotate('', xy=(1.5, 9), xytext=(2, 10), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(3.8, 9), xytext=(3.5, 10), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(6.6, 9), xytext=(6.5, 10), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(8.9, 9), xytext=(8, 10), arrowprops=arrow_props)\n",
    "\n",
    "# Models to ensemble\n",
    "ax.annotate('', xy=(3, 7), xytext=(1.5, 8), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(4, 7), xytext=(3.8, 8), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(6, 7), xytext=(6.6, 8), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(7, 7), xytext=(8.9, 8), arrowprops=arrow_props)\n",
    "\n",
    "# Ensemble to reconciliation\n",
    "ax.annotate('', xy=(5, 5), xytext=(5, 6), arrowprops=arrow_props)\n",
    "\n",
    "# Reconciliation to outputs\n",
    "ax.annotate('', xy=(2.7, 3), xytext=(4, 4), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(7.3, 3), xytext=(6, 4), arrowprops=arrow_props)\n",
    "\n",
    "# Add title\n",
    "ax.text(5, 11.5, 'Hierarchical Ensemble Forecasting with Uncertainty Quantification', \n",
    "        ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    mpatches.Patch(color=colors['data'], label='Data Layer'),\n",
    "    mpatches.Patch(color=colors['statistical'], label='Statistical Models'),\n",
    "    mpatches.Patch(color=colors['deep_learning'], label='Deep Learning Models'),\n",
    "    mpatches.Patch(color=colors['ensemble'], label='Ensemble Layer'),\n",
    "    mpatches.Patch(color=colors['reconciliation'], label='Reconciliation Layer'),\n",
    "    mpatches.Patch(color=colors['output'], label='Output Layer')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right', bbox_to_anchor=(1, -0.1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Model Architecture Features:\")\n",
    "print(\"1. Multi-model ensemble combining statistical and deep learning approaches\")\n",
    "print(\"2. Probabilistic reconciliation maintains forecast coherence\")\n",
    "print(\"3. Uncertainty quantification preserved through all hierarchy levels\")\n",
    "print(\"4. Scalable to large hierarchical datasets like M5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics and insights\n",
    "print(\"=\" * 60)\n",
    "print(\"HIERARCHICAL FORECAST RECONCILIATION - KEY INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nðŸŽ¯ PROJECT OBJECTIVES:\")\n",
    "print(\"  âœ“ Combine multiple forecasting approaches (statistical + deep learning)\")\n",
    "print(\"  âœ“ Maintain hierarchical coherence across all aggregation levels\")\n",
    "print(\"  âœ“ Preserve uncertainty quantification through reconciliation\")\n",
    "print(\"  âœ“ Achieve target performance metrics (WRMSSE < 0.52)\")\n",
    "\n",
    "print(\"\\nðŸ“Š DATA CHARACTERISTICS:\")\n",
    "print(f\"  â€¢ Total series across hierarchy: {total_series:,}\")\n",
    "print(f\"  â€¢ Time series length: {len(daily_total)} days\")\n",
    "print(f\"  â€¢ Hierarchy levels: {len(config['data']['aggregation_levels'])}\")\n",
    "print(f\"  â€¢ Zero sales rate: {(synthetic_data['sales'] == 0).mean():.1%}\")\n",
    "\n",
    "print(\"\\nðŸ”§ MODEL COMPONENTS:\")\n",
    "print(\"  Statistical Models:\")\n",
    "print(\"    - ETS (Exponential Smoothing) with seasonal patterns\")\n",
    "print(\"    - ARIMA with seasonal components\")\n",
    "print(\"  Deep Learning Models:\")\n",
    "print(\"    - Temporal Fusion Transformer (attention-based)\")\n",
    "print(\"    - N-BEATS (neural basis expansion)\")\n",
    "print(\"  Reconciliation:\")\n",
    "print(\"    - Minimum Trace with probabilistic constraints\")\n",
    "print(\"    - Uncertainty preservation through the hierarchy\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ TARGET METRICS:\")\n",
    "target_metrics_display = {\n",
    "    \"WRMSSE\": \"< 0.52 (Weighted Root Mean Squared Scaled Error)\",\n",
    "    \"Coverage 90%\": \"> 0.88 (90% prediction interval coverage)\", \n",
    "    \"Reconciliation Coherence\": \"> 0.99 (Hierarchical coherence score)\",\n",
    "    \"CRPS\": \"< 0.045 (Continuous Ranked Probability Score)\"\n",
    "}\n",
    "\n",
    "for metric, description in target_metrics_display.items():\n",
    "    print(f\"  â€¢ {metric}: {description}\")\n",
    "\n",
    "print(\"\\nðŸš€ INNOVATIONS:\")\n",
    "print(\"  1. Novel probabilistic reconciliation preserving uncertainty\")\n",
    "print(\"  2. Ensemble approach combining diverse model types\")\n",
    "print(\"  3. End-to-end pipeline with automated hyperparameter optimization\")\n",
    "print(\"  4. Comprehensive evaluation framework with multiple metrics\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ BUSINESS IMPACT:\")\n",
    "print(\"  â€¢ Improved demand planning accuracy across all business levels\")\n",
    "print(\"  â€¢ Reliable uncertainty estimates for risk management\")\n",
    "print(\"  â€¢ Scalable framework for large retail hierarchies\")\n",
    "print(\"  â€¢ Coherent forecasts supporting consistent decision-making\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To use this framework with real M5 data:\n",
    "\n",
    "1. **Download M5 Data**: Get the M5 forecasting competition dataset from Kaggle\n",
    "2. **Configure Paths**: Update the `data.path` in `configs/default.yaml`\n",
    "3. **Train Models**: Run `python scripts/train.py --optimize-hyperparameters`\n",
    "4. **Evaluate Results**: Use `python scripts/evaluate.py --create-plots --save-predictions`\n",
    "5. **Monitor Training**: View results in MLflow UI with `mlflow ui`\n",
    "\n",
    "The framework is designed to handle the full M5 dataset with 42,840 time series across 11 hierarchy levels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}