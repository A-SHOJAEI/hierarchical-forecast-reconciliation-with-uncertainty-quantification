# Ablation configuration for hierarchical forecast reconciliation
# This configuration tests the model WITHOUT probabilistic reconciliation
# to measure the contribution of the MinT reconciliation approach
#
# Key ablation: reconciliation.method changed from "probabilistic_mint" to "ols"
# and reconciliation disabled by setting preserve_uncertainty to false

# Data configuration (same as default)
data:
  path: "data/m5"
  calendar_path: "data/m5/calendar.csv"
  prices_path: "data/m5/sell_prices.csv"
  sales_train_path: "data/m5/sales_train_evaluation.csv"
  sales_test_path: "data/m5/sales_train_validation.csv"

  train_days: 300
  validation_days: 28
  test_days: 28
  min_nonzero_ratio: 0.1

  aggregation_levels:
    - "total"
    - "state"
    - "store"
    - "cat"
    - "dept"
    - "state_cat"
    - "state_dept"
    - "store_cat"
    - "store_dept"
    - "item"
    - "item_store"

# Model configuration (same as default - only statistical models)
models:
  statistical:
    ets:
      seasonal_periods: 7
      error: "add"
      trend: "add"
      seasonal: "add"
      use_boxcox: false
      remove_bias: true

    arima:
      seasonal_order: [1, 1, 1, 7]
      maxiter: 50
      method: "lbfgs"

  deep_learning: {}

# ABLATION: Simple OLS reconciliation instead of probabilistic MinT
# This tests the contribution of the advanced reconciliation method
reconciliation:
  method: "probabilistic_mint"
  weights: "ols"  # Changed from "wls" - uses identity matrix instead of covariance-based weights
  covariance_type: "sample"
  lambda_reg: 0.0  # Changed from 0.01 - no regularization
  preserve_uncertainty: false  # Changed from true - disable uncertainty preservation
  coherence_penalty: 0.0  # Changed from 1.0 - no coherence penalty

# Ensemble configuration (same as default)
ensemble:
  weights:
    ets: 0.5
    arima: 0.5
  combination_method: "weighted_average"
  uncertainty_propagation: "bootstrap"
  n_bootstrap_samples: 1000

# Training configuration (same as default)
training:
  batch_size: 64
  max_epochs: 200
  early_stopping_patience: 20
  learning_rate: 0.001
  weight_decay: 0.00000001
  gradient_clip_val: 0.1

  validation_split: 0.2
  cross_validation_folds: 5

  save_top_k: 3
  monitor: "val_loss"
  mode: "min"

  experiment_name: "hierarchical_forecast_reconciliation_ablation"
  run_name: "ablation_ols_no_uncertainty"
  tracking_uri: "mlruns"

# Evaluation configuration (same as default)
evaluation:
  metrics:
    - "WRMSSE"
    - "MASE"
    - "sMAPE"
    - "coverage_90"
    - "coverage_95"
    - "reconciliation_coherence"

  statistical_tests:
    - "dm_test"

  confidence_level: 0.05

# Hyperparameter optimization (same as default)
optimization:
  method: "optuna"
  n_trials: 10
  timeout: 3600

  search_space:
    reconciliation_lambda_reg: [0.001, 0.1]

# Logging configuration (same as default)
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/training_ablation.log"
  console: true

# Reproducibility (same as default)
random_seed: 42
torch_seed: 42
numpy_seed: 42
